# LLM Benchmark Suite for Humanities Image Data
This repository contains a number of benchmark datasets (images), prompts, ground truths and evaluation scripts for
evaluating the performance of large language models (LLMs) on tasks in the humanities. The repository is
intended to be a resource for researchers and practitioners who are interested in evaluating the performance
of LLMs on DH-related tasks.

## Datasets
The repository contains the following datasets:
- [**Test Benchmark 1**](benchmarks/test_benchmark/README.md): A simple workflow without any additional configuration.
- [**Test Benchmark 2**](benchmarks/test_benchmark2/README.md): A simple workflow with configured Benchmark class.
- [**Bibliographic Data**](benchmarks/bibliographic_data/README.md): A benchmark for extracting bibliographic data from images.


## Evaluation


## Use the Benchmark Suite
To use the benchmark suite, you can clone the repository and add benchmarks to the benchmark folder like so:

### Create a new benchmark
To create a new benchmark, follow these steps:

- Create a new directory in the `benchmarks` folder with the name of the benchmark. 
The directory should have the following structure:

```bash
.
├── benchmarks
│   ├── <new_benchmark_name>
│   │   ├── README.md
│   │   ├── result_table.md (generated)
│   │   ├── benchmark.py (optional)
│   │   ├── images
│   │   │   ├── image1.(jpg|png)
│   │   │   ├── image2.(jpg|png)
│   │   │   ├── ...
│   │   ├── prompts
│   │   │   ├── prompt1.txt
│   │   │   ├── prompt2.txt
│   │   │   ├── ...
│   │   ├── ground_truths
│   │   │   ├── image1.(json|txt)
│   │   │   ├── image2.(json|txt)
│   │   │   ├── ...
│   │   ├── results
│   │   │   ├── image1.json
│   │   │   ├── image2.json
│   │   │   ├── ...
```

- Create a `README.md` file with a description of the benchmark. Make sure to include a dataset description
and a description of the task that the model should perform. Explain your evaluation criteria and how the
results are scored. You can include the generated result table in this file: `![Result Table](result_table.md)`
- You don't need to create the `result_table.md` file, it will be generated when running the benchmark.
- Add images to the `images` directory. You need at least one image. Per default, one image is used with one request.
If you want to use multiple images for one prompt, format the image names like this: `image1_p1.jpg`, `image1_p2.jpg`, etc. 
This will automatically add multiple images to the same prompt. The ground truth file for this image sequence must be 
named like so: `image1.(txt|json)`.
- Add at least one prompt to the `prompts` directory. This is a simple text file with the prompt for the model.
- Add the ground truth for each image (or image sequence) to the `ground_truths` directory.
- The results are generated by the model and saved in the `results` directory. The results are saved in a JSON file

This setup will enable you to add configurations to the ```benchmarcks.csv``` file and run the benchmark with different
LLMs, prompts, and configurations.

### API Keys
To use the benchmark suite, you need to provide API keys for the different providers.
- Create a `.env` file in the root directory of the repository.
- Add the following lines to the `.env` file:

```bash
OPENAI_API_KEY=<your_openai_api_key>
GENAI_API_KEY=<your_genai_api_key>
ANTHROPIC_API_KEY=<your_anthropic_api_key>
```

### Add Configuration
To add a configuration, you need to add a new row to the `benchmarks.csv` file. The file has the following structure:

```csv
name,provider,model,role_description,prompt_file,legacy_test
test_benchmark,openai,gpt-40,You are a professsor,prompt1.txt,False
```

- `name`: The name of the benchmark. This must match the name of the directory in the `benchmarks` folder.
- `provider`: The provider of the model. This can be `openai`, `genai`, or `anthropic`.
- `model`: The name of the model. This can be any model name that is supported by the provider.
- `role_description`: A description of the role that the model should take on. This can be any description that is supported by the provider.
- `prompt_file`: The name of the prompt file in the `prompts` directory.
- `legacy_test`: A boolean value that indicates whether the benchmark is a legacy test. This can be `True` or `False`.

This allows you to run the benchmark with different models, prompts, and configurations.

### Implement a Benchmark Class
If you want to implement a custom benchmark class, you can create a `benchmark.py` file in the benchmark directory.
This file must contain a class that inherits from the `Benchmark` class. The class must be named like the benchmark 
folder but in CamelCase. Example: `TestBenchmark` for the `test_benchmark` folder.

The class can implement the following methods:

```python
from scripts.benchmark_base import Benchmark

class TestBenchmark(Benchmark):

    def score_answer(self, image_name, response, ground_truth):
        data = self.prepare_scoring_data(response)  # Extracts the relevant data from the response and returns a dictionary
        # [...] Score the answer from the model
        return {"total": 0} # Return a dictionary with scores   
        
    @property
    def convert_result_to_json(self):
        return True

    @property
    def resize_images(self):
        return True

    @property
    def get_page_part_regex(self):
        return r'(.+)_p\d+\.(jpg|jpeg|png)$'

    @property
    def get_output_format(self):
        return "json"
    
    @property
    def title(self):
        return f"{self.name} ({self.provider}/{self.model})"
```

The `score_answer` method is used to score the answer from the model. The method receives the image name, the response
from the model, and the ground truth. The method should return a dictionary with the scores. The keys of the dictionary
should be the names of the evaluation criteria, and the values should be the scores.

The rest of the methods are properties that can be used to configure the behavior of the benchmark. 
The `convert_result_to_json` property indicates whether the results should be converted to JSON format.
The `resize_images` property indicates whether the images should be resized before being sent to the model.
The `get_page_part_regex` property is a regular expression that is used to extract the page part from the image name.
The `get_output_format` property indicates the output format of the model response.
The `title` property is used to generate the title of the benchmark.
